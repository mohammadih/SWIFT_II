# SWIFT-II Spectrum Coexistence Framework

This repository contains the MATLAB and Python co-simulation environment developed for the **SWIFT-II** project, enabling deep reinforcement learning (DRL) assisted spectrum sharing between 5G New Radio (NR) systems and passive RF sensors.

## Overview of Proposed Structure and Model

The framework implements a **Deep Reinforcement Learningâ€“assisted dynamic spectrum selection** pipeline that supports coexistence between 5G active systems and passive RF sensors within a shared spectrum band. The solution targets **AI-driven spectrum management** for **remote sensing (RS) protection** and **interference-resilient 5G communications**.

### System Architecture

Three primary components interact in the environment:

1. **5G Transceiver (gNB/UE Pair):** Generates and transmits 5G NR-compliant waveforms within the shared band.
2. **Passive Sensor Node:** Continuously senses the spectral environment and reports **power spectral density (PSD)** measurements.
3. **DRL-Based Spectrum Agent:** Serves as the decision-making controller that dynamically allocates or avoids frequency channels to protect passive sensors.

The environment is modeled as a **Markov Decision Process (MDP)** with:

- **State (`s_t`):** Spectral occupancy features, interference-level estimates, and channel quality indicators.
- **Action (`a_t`):** Selection of sub-bands or resource block (RB) groups for 5G transmission.
- **Reward (`r_t`):** Balances 5G throughput, interference reduction at the passive sensor, and sensing accuracy preservation.

### AI and Learning Framework

A **Double Deep Q-Network (DDQN)** is employed to mitigate Q-value overestimation and stabilize convergence. The agent observes time-varying spectrum states via CNN-based feature extraction, learns optimal access policies through experience replay, and leverages target network updates for robustness. The objective is:

\\[
\max_{\pi} \mathbb{E} \left[\sum_{t=0}^{T} \gamma^t \left(R_{5G}(a_t,s_t) - \lambda P_{\text{int}}(a_t,s_t)\right) \right]
\\]

subject to spectrum coexistence and latency constraints, where `R_{5G}` denotes throughput gain and `P_{int}` represents interference at the passive sensor.

### Simulation and Evaluation

The co-simulation couples **MATLAB** (5G physical-layer modeling) with **Python/TensorFlow** (DRL training). Shared-spectrum dynamics are emulated using multi-band PSD data, SINR mapping, and CDL/TDL fading models. Results demonstrate:

- Adaptive avoidance behavior without explicit coordination;
- Up to **40â€“50% interference mitigation** with minimal throughput degradation;
- Faster convergence and better sensing protection than heuristic baselines.

## Algorithmic Workflow: DRL-Assisted Spectrum Selection

### 1. Environment Definition (Hybrid PHY + RL Layer)

Each time step represents a transmission interval or sensing cycle within the shared spectrum.

- `ğ”½ = {fâ‚, fâ‚‚, â€¦, f_N}` â€” available sub-bands / RB groups  
- `P_tx` â€” 5G transmit power  
- `H(f_i, t)` â€” sub-band channel response (from MATLAB channel model)  
- `PSD(f_i, t)` â€” passive sensor power spectral density measurements  
- `ÏƒÂ²` â€” thermal noise level  
- `Ï„` â€” sensing feedback latency  

**RL Observations**

- Spectrum occupancy map `S_t = [PSD(fâ‚,t), â€¦, PSD(f_N,t)]`  
- CQI derived from SINR estimates  
- Interference ratio `I_t = P_int / P_th`

### 2. State, Action, and Reward Design

| Element            | Description                                                                 | Expression                                      |
| ------------------ | --------------------------------------------------------------------------- | ----------------------------------------------- |
| **State** `s_t`    | Concatenated spectral and interference features                             | `s_t = [PSD_t, CQI_t, I_t]`                      |
| **Action** `a_t`   | Selects sub-band(s) for 5G transmission                                     | `a_t âˆˆ ğ”½`                                       |
| **Reward** `r_t`   | Balances throughput against interference                                    | `r_t = Î± logâ‚‚(1 + Î³_t) - Î² I_t`                 |
| **Transition**     | Environment evolves according to spectrum and channel dynamics              | `s_{t+1} = f(s_t, a_t, H_t)`                     |

### 3. Deep RL Agent: Double Deep Q-Network

Two networks are trained:

- **Online:** `Q_Î¸(s_t, a_t)`  
- **Target:** `Q_{Î¸â»}(s_t, a_t)` updated every `C` steps

Loss function:

\\[
L(\theta) = \mathbb{E} \left[ \left( r_t + \gamma Q_{\theta^-}\left(s_{t+1}, \arg\max_{a'} Q_{\theta}(s_{t+1},a')\right) - Q_{\theta}(s_t,a_t) \right)^2 \right]
\\]

Gradient descent update: `Î¸ â† Î¸ - Î· âˆ‡_Î¸ L(Î¸)`

Exploration: Îµ-greedy with exponential decay `Îµ_t = max(Îµ_min, Îµ_0 e^{-k t})` (optional Boltzmann exploration).

### 4. Algorithm Pseudocode

```python
Initialize environment E (MATLAB channel + PSD sensing)
Initialize replay buffer D with capacity M
Initialize DDQN networks QÎ¸, QÎ¸âˆ’ with random weights
Set Îµ = Îµ0, learning rate Î·, discount Î³, target update C

for episode in range(N_episodes):
    s = E.reset()
    for t in range(T):
        if random() < Îµ:
            a = random_action()
        else:
            a = argmax_a QÎ¸(s, a)

        throughput, interference, PSD_next = simulate_PHY(a)
        r = Î± * log2(1 + throughput) - Î² * interference
        s_next = concat(PSD_next, CQI_next, interference)
        D.append((s, a, r, s_next))

        if len(D) > batch_size:
            batch = random_sample(D)
            update_Q_networks(batch)

        if t % C == 0:
            Î¸âˆ’ â† Î¸

        s = s_next

    Îµ = decay(Îµ)
```

### 5. Simulation Loop and MATLAB Interface

- **MATLAB Layer:** Implements the 5G NR PHY (OFDM waveform generation, CDL channel models, spectrum analysis) and returns key KPIs:

  ```matlab
  [throughput, PSD_next, CQI_next, interference] = PHY_simulate(action, params);
  ```

- **Python Layer:** Handles DDQN learning, replay memory, model persistence, and TensorBoard logging.

### 6. Output Metrics and Analysis

- Throughput gain `Î”R = ((R_DRL - R_baseline) / R_baseline) Ã— 100%`
- Interference reduction `Î”P_int = ((P_baseline - P_DRL) / P_baseline) Ã— 100%`
- Learning stability via average Q-value variance `Ïƒ_QÂ²`
- Convergence rate: episode index achieving 90% of peak reward

### 7. Conceptual Block Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Spectrum Environment      â”‚
â”‚  (5G TX + Passive Sensor)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ PSD + CQI feedback
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    DRL Agent (DDQN)          â”‚
â”‚  State â†’ Action (sub-band)   â”‚
â”‚  Reward â† Interference & R5G â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Action
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MATLAB PHY Simulation Layer  â”‚
â”‚  OFDM + Channel + RFI model  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Updated PSD
        â†â”€â”€â”€â”€â”€â”€â”˜
```

### 8. References

- Sutton & Barto, *Reinforcement Learning: An Introduction*, 2018  
- Haykin, *Cognitive Dynamic Systems: Perception-Action Cycle*, 2008  
- Hong et al., *Federated Edge Learning for 6G*, 2021
